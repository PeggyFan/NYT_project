{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sklearn.cluster\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import lstsq\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "br trump like gop amp said imagine mr national oh\n",
      "Topic #2:\n",
      "people just time don work like money government think need\n",
      "Topic #3:\n",
      "com http www 2015 href _blank target title nytimes 07\n",
      "Topic #4:\n",
      "clinton hillary trump biden president republican joe run party vote\n",
      "Topic #5:\n",
      "bernie sanders hillary candidate people support times article nyt nate\n"
     ]
    }
   ],
   "source": [
    "hilary = pd.read_csv('data/hilary_scores.csv')\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(hilary['Comment'].values).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=5).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "    print('################################################')\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "br war paul people great right democrats question times wrong\n",
      "Topic #2:\n",
      "bernie sanders senator thank love socialist vote candidate hero independent\n",
      "Topic #3:\n",
      "clinton hillary president obama vote republican party candidate republicans democratic\n",
      "Topic #4:\n",
      "com http www href _blank target title org html crowdfunding\n",
      "Topic #5:\n",
      "people tax government money just pay like class don middle\n"
     ]
    }
   ],
   "source": [
    "sanders = pd.read_csv('data/sanders_scores.csv')\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(sanders['Comment'].values).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=5).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "br trump com people http make sharpton democrats government republican\n",
      "Topic #2:\n",
      "biden joe run president presidential vice politician warren honest legacy\n",
      "Topic #3:\n",
      "president obama people like just don mr think time country\n",
      "Topic #4:\n",
      "sanders bernie candidate mention biden running nyt nate black cohn\n",
      "Topic #5:\n",
      "hillary clinton vote bush email 2016 obama mrs democratic election\n"
     ]
    }
   ],
   "source": [
    "biden = pd.read_csv('data/biden_scores.csv')\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(biden['Comment'].values).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=5).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump = pd.read_csv('data/trump_scores.csv')\n",
    "pd.isnull(trump['Comment']).sum()\n",
    "# vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "# V = vectorizer.fit_transform(trump['Comment'].values).toarray() \n",
    "# features = vectorizer.get_feature_names()\n",
    "# nmf = NMF(n_components=5).fit(V)\n",
    "\n",
    "# for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "#     print(\"Topic #%d:\" % topic_idx)\n",
    "#     print(\" \".join([features[i]\n",
    "#                     for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "br rubio donald right said walker paul race did big\n",
      "Topic #2:\n",
      "jeb bush brother george did family clinton florida know house\n",
      "Topic #3:\n",
      "trump donald republican gop mr party media candidates debate like\n",
      "Topic #4:\n",
      "people like just don republican money time president republicans think\n",
      "Topic #5:\n",
      "com www http href _blank title target https 2015 bernie\n"
     ]
    }
   ],
   "source": [
    "bush = pd.read_csv('data/bush_scores.csv')\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(bush['Comment'].values).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=10).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1951c2df5657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 804\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 236\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    117\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "carson = pd.read_csv('data/carson_scores.csv')\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(carson['Comment'].values).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=5).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
