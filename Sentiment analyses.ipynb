{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import stem\n",
    "snowball = stem.snowball.EnglishStemmer()\n",
    "\n",
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.15625, 1.1607142857142858, 0.4166666666666667, 0.5514705882352942, -0.08928571428571429]\n",
    "\n",
    "[-0.15625, 0.0, -0.05, -0.03676470588235294, -0.08928571428571429]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitter = Splitter()\n",
    "postagger = POSTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitted_sentences = splitter.split(text)\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lexicons = []\n",
    "# with open(\"SentiWordNet.txt\") as f:\n",
    "#     for i in f.readlines():\n",
    "#         lexicons.append(i)\n",
    "# lexicons = lexicons[27:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "def create_dict(text):\n",
    "    pos_dict = defaultdict(int) \n",
    "    neg_dict = defaultdict(int)\n",
    "    for i in range(len(text)):\n",
    "        word = text[i].split(\"\\t\")[4].split(\"#\")[0]\n",
    "        if word == '':\n",
    "            pass\n",
    "        else:\n",
    "            pos_dict[word] = float(text[i].split(\"\\t\")[2])\n",
    "            neg_dict[word] = float(text[i].split(\"\\t\")[3])     \n",
    "        \n",
    "    return pos_dict, neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_dict = create_dict(lexicons)[0]\n",
    "neg_dict = create_dict(lexicons)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'nice': 0.25}"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= {\"nicely\": 0.25}\n",
    "dict([(snowball.stem(k), v) for k, v in y.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dict = dict([(snowball.stem(k), v) for k, v in pos_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dict_B = {snowball.stem(k): v for k, v in pos_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'nicely', 0.25), (u'niceness', 0.625), (u'nice', 0.0)]"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "stem_words = defaultdict(list)\n",
    "\n",
    "for k, v in pos_dict.items():\n",
    "    stem_words[snowball.stem(k)].append((k, v))\n",
    "\n",
    "stem_words['nice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use sorted dictionaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use sorted dictionaries?\n",
    "#pos_dict_sorted = OrderedDict(sorted(pos_dict.items(), key=lambda t: t[0]))\n",
    "#neg_dict_sorted = OrderedDict(sorted(neg_dict.items(), key=lambda t: t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(pos_dict, open(\"dicts/pos_dict\", 'w'))\n",
    "json.dump(neg_dict, open(\"dicts/neg_dict\", 'w'))\n",
    "\n",
    "pos_dict = json.load(open('dicts/pos_dict'))\n",
    "neg_dict = json.load(open('dicts/neg_dict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(word):\n",
    "    snowball = stem.snowball.EnglishStemmer()\n",
    "    return snowball.stem(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cal_senti_score(pos_tagged_sentences):\n",
    "    score = []\n",
    "    \n",
    "    for sent in pos_tagged_sentences:\n",
    "        pos_score = []\n",
    "        neg_score = []\n",
    "        total = 0 \n",
    "\n",
    "        total = len(sent)\n",
    "        \n",
    "        for token in sent:\n",
    "            if pos_dict.get(token[0]) == None:\n",
    "                pass\n",
    "            else:\n",
    "                #token[2].append(pos_dict[token[0]])\n",
    "                pos_score.append(pos_dict[token[0]])\n",
    "        for token in sent:\n",
    "            if neg_dict.get(token[0]) == None:\n",
    "                pass\n",
    "            else:\n",
    "                #token[2].append(neg_dict[token[0]])\n",
    "                neg_score.append(neg_dict[token[0]])\n",
    "        sent_score = (sum(pos_score) - sum(neg_score)) / float(total) \n",
    "        print pos_score\n",
    "        print neg_score\n",
    "        \n",
    "        score.append(sent_score)\n",
    "    return sum(score) / float(len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.625, 0.625]\n",
      "[0.25, 0.375, 0.0, 0.0, 0.5]\n",
      "[0.0, 0.5, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.375, 0.25, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.5, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.009733893557422971"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_senti_score(pos_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "#print pos_dict['not']\n",
    "print pos_dict['very']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text =\"What can I say about this place. \\\n",
    "The staff of the restaurant is nice and the eggplant is not bad. \\\n",
    "Apart from that, very uninspired food, lack of atmosphere and too expensive. \\\n",
    "I am a staunch vegetarian and was sorely dissapointed with the veggie options on the menu. \\\n",
    "Will be the last time I visit, I recommend others to avoid.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to account for incrementers:\n",
    "\"too\", \"very\", \"sorely\"\n",
    "decrementers:\n",
    "\"barely\", \"little\"\n",
    "inverters:\n",
    "\"lack of\"\n",
    "\"not\"\n",
    "\n",
    "Needs to remove stopwords from text, then tag the sentences?\n",
    "Needs to stem the dictionary, but how?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
