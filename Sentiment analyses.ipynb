{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import stem\n",
    "snowball = stem.snowball.EnglishStemmer()\n",
    "\n",
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text =\"What can I say about this place. \\\n",
    "The staff of the restaurant is nice and the eggplant is not bad. \\\n",
    "Apart from that, very uninspired food, lack of atmosphere and too expensive. \\\n",
    "I am a staunch vegetarian and was sorely dissapointed with the veggie options on the menu. \\\n",
    "Will be the last time I visit, I recommend others to avoid.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-0.15625, 1.1607142857142858, 0.4166666666666667, 0.5514705882352942, -0.08928571428571429]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitter = Splitter()\n",
    "postagger = POSTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitted_sentences = splitter.split(text)\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lexicons = []\n",
    "# with open(\"SentiWordNet.txt\") as f:\n",
    "#     for i in f.readlines():\n",
    "#         lexicons.append(i)\n",
    "# lexicons = lexicons[27:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "def create_dict(text):\n",
    "    pos_dict = defaultdict(int) \n",
    "    neg_dict = defaultdict(int)\n",
    "    for i in range(len(text)):\n",
    "        word = text[i].split(\"\\t\")[4].split(\"#\")[0]\n",
    "        if word == '':\n",
    "            pass\n",
    "        else:\n",
    "            pos_dict[word] = float(text[i].split(\"\\t\")[2])\n",
    "            neg_dict[word] = float(text[i].split(\"\\t\")[3])     \n",
    "        \n",
    "    return pos_dict, neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_dict = create_dict(lexicons)[0]\n",
    "neg_dict = create_dict(lexicons)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'nice': 0.25}"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= {\"nicely\": 0.25}\n",
    "dict([(snowball.stem(k), v) for k, v in y.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dict = dict([(snowball.stem(k), v) for k, v in pos_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dict_B = {snowball.stem(k): v for k, v in pos_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'nicely', 0.25), (u'niceness', 0.625), (u'nice', 0.0)]"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "stem_words = defaultdict(list)\n",
    "\n",
    "for k, v in pos_dict.items():\n",
    "    stem_words[snowball.stem(k)].append((k, v))\n",
    "\n",
    "stem_words['nice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pos_dict_sorted = OrderedDict(sorted(pos_dict.items(), key=lambda t: t[0]))\n",
    "# neg_dict_sorted = OrderedDict(sorted(neg_dict.items(), key=lambda t: t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(pos_dict, open(\"dicts/pos_dict\", 'w'))\n",
    "json.dump(neg_dict, open(\"dicts/neg_dict\", 'w'))\n",
    "\n",
    "# reading\n",
    "#test = json.load(open('pos_dict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#files = [open(path, 'r') for path in ['dicts/pos_dict', 'dicts/neg_dict']]\n",
    "#dictionaries = [json.load(open(dict_file)) for dict_file in files]\n",
    "pos_dict = json.load(open('dicts/pos_dict'))\n",
    "neg_dict = json.load(open('dicts/neg_dict'))\n",
    "dictionaries = [pos_dict, neg_dict]\n",
    "\n",
    "# the problem is that I have two values, degree of positive and negative, for each word.\n",
    "# instead of just having positive and negative as labels.\n",
    "map(lambda x: x.close(), files)\n",
    "dictionary = neg_dict\n",
    "max_key_size = 0\n",
    "\n",
    "for key in dictionary:\n",
    "    max_key_size = max(max_key_size, len(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15625, 0.0, -0.05, -0.03676470588235294, -0.08928571428571429]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(word):\n",
    "    snowball = stem.snowball.EnglishStemmer()\n",
    "    return snowball.stem(word.lower())\n",
    "\n",
    "score = []\n",
    "\n",
    "for sent in pos_tagged_sentences:\n",
    "    pos_score = []\n",
    "    neg_score = []\n",
    "    total = 0 \n",
    "    \n",
    "    total += len(sent)\n",
    "    for token in sent:\n",
    "        if pos_dict.get(token[0]) == None:\n",
    "            pass\n",
    "        else:\n",
    "            token[2].append(dictionary[token[0]])\n",
    "            pos_score.append(dictionary[token[0]])\n",
    "    for token in sent2:\n",
    "        if neg_dict.get(token[0]) == None:\n",
    "            pass\n",
    "        else:\n",
    "            token[2].append(dictionary[token[0]])\n",
    "            neg_score.append(dictionary[token[0]])\n",
    "    sent_score = (sum(pos_score) - sum(neg_score)) / float(total) \n",
    "    score.append(sent_score)\n",
    "    \n",
    "print score\n",
    "senti_score = sum(score) / float(len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06646008403361345"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
